
# Документация по сбору и подготовке датасета

## Цель проекта

Создание системы для тематической классификации коротких текстов (2–30 слов), способной определить принадлежность текста к одной или нескольким тематикам из набора:

- спорт
- юмор
- реклама
- соцсети
- политика
- личная_жизнь

Формат задачи: **одноклассовая классификация** (multi-class), где каждому тексту присваивается одна наиболее вероятная тема.

---

## Этап 1: Автоматическая разметка (zero-shot классификация)

### Входные данные

6 CSV-файлов (`1.csv` – `6.csv`) с короткими текстами, каждый из которых предположительно соответствует одной из тем.

### Модель

- **Pipeline:** `transformers.pipeline("zero-shot-classification")`
- **Модель:** `joeddav/xlm-roberta-large-xnli`
- **multi_label:** False
- **Порог:** отсутствует — выбирается одна наиболее вероятная метка
- **labels:** ['спорт', 'юмор', 'реклама', 'соцсети', 'политика', 'личная_жизнь']

### Результат

Получен файл `labeled_dataset_zero.csv` с автоматически размеченными текстами.  
Примеры меток:
- `1.csv` → соцсети (60%)
- `5.csv` → спорт (52%)
- `6.csv` → юмор (72%)
- `2, 3, 4.csv` — без явного преобладания метки

---


---

## Обоснование выбора одноклассовой классификации

Хотя изначально задача может рассматриваться как многоклассовая (multi-label), в данном проекте использован **одноклассовый подход** (`multi_label=False`) при помощи модели `joeddav/xlm-roberta-large-xnli`.

### Выбор одноклассового режима:

- Модель zero-shot классификации выдаёт **одну метку с наивысшей уверенностью**.
- Указание `multi_label=False` позволяет упростить обработку и избежать неопределённости в интерпретации.
- Во многих практических случаях короткий текст действительно описывает **одну доминирующую тему**.
- Это позволило:
  - упростить автоматическую разметку;
  - избежать пороговой настройки confidence;
  - ускорить балансировку и построение финального датасета.

Применение такого подхода оправдано в ситуациях, когда приоритетом явзяется **простота и воспроизводимость классификации**, особенно в условиях ограниченных ресурсов.


## Этап 2: Предобработка данных

### Удаление дубликатов
```python
df = df.drop_duplicates(subset=['text'])
```

### Очистка текста
```python
def clean_text(text):
    text = re.sub(r"http\S+", "", text)
    text = re.sub(r"[^\w\s]", "", text)
    text = re.sub(r"\s+", " ", text).strip()
    return text.lower()
```

### Фильтрация по длине
```python
df["word_count"] = df["text"].apply(lambda x: len(x.split()))
df = df[(df["word_count"] >= 2) & (df["word_count"] <= 30)]
```

---

## Обработка меток

```python
from sklearn.preprocessing import MultiLabelBinarizer
mlb = MultiLabelBinarizer()
y = mlb.fit_transform(df["label"].apply(lambda x: [x]))
```

---

## Балансировка выборки

### Проблема
После разметки — сильный дисбаланс по меткам:

| Метка          | Кол-во |
|----------------|--------|
| юмор           | 725    |
| соцсети        | 557    |
| спорт          | 355    |
| политика       | 301    |
| личная_жизнь   | 57     |
| реклама        | 53     |

### Стратегия решения проблемы дисбаланса по меткам

#### Для меток с избытком:
- отбор по `score > 0.5` (уверенность модели)

```python
df_confident = df[df['score'] > 0.5]
```

#### Для меток с дефицитом:
- **oversampling + аугментация**

```python
def oversample_with_augmentation(df, target_samples, text_column='text', label_column='label', drop_prob=0.2):
    ...
```

Цель — выровнять количество записей.

---


## Финальный датасет

После балансировки каждый класс был доведён до примерно **900–1000 строк**, чтобы устранить дисбаланс, но сохранить как можно больше информации из сильных источников данных.

| Метка          | После балансировки |
|----------------|--------------------|
| политика       | 986                |
| реклама        | 973                |
| юмор           | 954                |
| личная_жизнь   | 937                |
| соцсети        | 934                |
| спорт          | 895                |


## Разделение выборки

```python
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y)
X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1, stratify=y_train)
```

---

## Обоснование отказа от внешнего парсинга

Во многих прикладных задачах (например, классификация обращений клиентов, внутренние документы) используются **уникальные, закрытые данные**, которые:

- нельзя пополнить извне;
- отличаются по стилю и тематике от общедоступных текстов.

В этом проекте мы сознательно **работали только с исходными 6 файлами**, чтобы смоделировать такую ситуацию и отработать:

- автоматическую разметку;
- аугментацию и балансировку;
- анализ качества на ограниченном датасете.

---

## Используемые библиотеки

- Hugging Face `transformers`
- Scikit-learn
- Pandas / NumPy
- Matplotlib / Seaborn

---

## Вывод

Реализована полноценная подготовка сбалансированного обучающего датасета на ограниченных данных, пригодного для многотематической классификации коротких текстов в условиях ограниченного доступа к внешним источникам.

---

## Обоснование использования только исходных 6 датасетов (без внешнего парсинга)

В рамках работы по подготовке датасета было принято принципиальное решение **не расширять обучающую выборку за счёт внешних источников** (например, открытых новостных лент, социальных сетей или форумов), несмотря на кажущуюся простоту их получения по заданным тематикам.

### Причина: имитация работы со специфичными, ограниченными данными

В практической ситуации, при решении задач для корпоративных или закрытых систем, обучение модели проводится на основе внутренних, ограниченных данных:

- внутренних баз данных клиента;
- внутренних переписок или CRM-сообщений;
- технической документации;
- отраслевых отзывов, не представленных в открытом доступе.

Эти данные:

- принципиально не могут быть дополнены из внешних источников из-за требований конфиденциальности или специфики формата;
- могут отличаться от данных из открытых источников по стилистике, специфической терминологии и принятым обозначениям, структуре.

### Практическая цель 

Данная задача предобработки данных имеет практическую цель - построить высококачественный классификатор и сбалансировать обучающий датасет, используя только те данные, которые изначально доступны (как в количественном, так и в качественном виде).

Что позволило:

- смоделировать реальную ситуацию;
- сосредоточиться на разметке, аугментации и аналитической работе с ограниченным корпусом данных;
- отработать методы, которые могут быть применениы в закрытой среде.
